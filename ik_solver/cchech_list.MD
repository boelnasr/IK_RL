# RL IK Project â€” Tune-up Checklist

> Tick the boxes â˜‘ as you complete each item.

---

## 0  Prep

- [ **X**] **Create a â€œdev-tuneâ€ branch** so *master* stays runnable  
- [ ] Run unit tests â†’ ensure baseline still passes before editing  

---

## 1  Quick Wins (â‰¤ 1 day)

| Done | Priority | Item | File / Function |
|------|----------|------|-----------------|
| [**X**] | ðŸ’¡ | Switch Bullet to **DIRECT** mode during training | `InverseKinematicsEnv.__init__` â†’ `p.connect(p.DIRECT)` |
| [ ] | ðŸ’¡ | Call `get_actions(..., eval_mode=False)` after warm-up **or** gradually set `pd_weight â†’ 0` | `MAPPOAgent.train()` |
| [**X** ] | ðŸ’¡ | Clamp orientation error to **Â± Ï€** before reward calc | `reward_function.compute_reward` |
| [ ] | ðŸ’¡ | Replace inner `for`-loops in trajectory build with `torch.stack` / `torch.cat` | `MAPPOAgent.train()` |

---

## 2  RL-side Tweaks (1â€“3 days)

| Done | Priority | Item | File / Function |
|------|----------|------|-----------------|
| [ **x**] | ðŸ”§ | Remove HER / Prioritized buffers from **on-policy** path | `MAPPOAgent.train()`, `update_policy_with_experiences` |
| [ ] | ðŸ”§ | **Share actor parameters** across joints (add `agent_id` embedding) | `models.JointActor` |
| [ **X**] | ðŸ”§ | Replace custom entropy adaptation with **target-KL schedule** | `MAPPOAgent.update_policy` |
| [ ] | ðŸ”§ | Add **soft target critic** (Ï„ â‰ˆ 0.005) for advantages | `MAPPOAgent.__init__`, `update_policy` |
| [ ] | ðŸ”§ | Insert gradient NaN / explode guard, skip update if triggered | `update_policy`, `update_policy_with_experiences` |

---

## 3  Environment & Curriculum (2â€“5 days)

| Done | Priority | Item | File / Function |
|------|----------|------|-----------------|
| [ ] | ðŸ”§ | Sample 20 % targets on workspace boundary, 10 % near singularities | `sample_valid_target_position` |
| [ ] | ðŸ”§ | Success threshold **per joint**: `Ïƒ_i = 0.1 rad Â· e^{-3Â·difficulty_i}` | `is_agent_success` |
| [ ] | ðŸ”§ | Parallelise roll-outs with **SubprocVecEnv / Ray** | `envs/vec_env.py`, training launcher |
| [ ] | ðŸ”§ | Add **domain randomisation** (mass Â± 10 %, friction, sensor noise) | `InverseKinematicsEnv.reset` |
| [ ] | ðŸ›  | Swap `POSITION_CONTROL` for **TORQUE_CONTROL + inner PD @ 1 kHz** | `InverseKinematicsEnv.step` |

---

## 4  Network & Loss Surgery (~1 week)

| Done | Priority | Item | File / Function |
|------|----------|------|-----------------|
| [ ] | ðŸ›  | Introduce **GNN encoder** (nodes = joints) | `models/gnn_encoder.py` |
| [ ] | ðŸ›  | Add auxiliary heads: predict Cartesian Î” & Jacobian cond-# | `CentralizedCritic.forward` |
| [ ] | ðŸ›  | Implement adaptive-KL curriculum (target KL â†‘ when success > 80 %) | `update_policy` |
| [ ] | ðŸ›  | Add Î»-return look-ahead (`t+T+1`) in advantage calc | `compute_individual_gae` |

---

## 5  Diagnostics & CI

| Done | Priority | Item | File / Function |
|------|----------|------|-----------------|
| [ ] | ðŸ’¡ | Enable TensorBoard/WandB: advantages, TD-error hist, PD vs RL magnitude | `training_metrics.log_episode` |
| [ ] | ðŸ’¡ | Save **worst-10** trajectories each epoch for regression tests | new util in `training_metrics` |
| [ ] | ðŸ’¡ | Unit-test analytic Jacobian vs numeric (< 1e-3 RMS) | `tests/test_jacobian.py` |
| [ ] | ðŸ’¡ | Add CLI flags `--no-pd`, `--pd-anneal=exp`, `--vectorised N` | `args.py` |

---

### How to use

1. Copy this file as `CHECKLIST.md` in your repo.  
2. Check off items as your commits land.  
3. After finishing each section, rerun unit tests and a 500-episode benchmark.
